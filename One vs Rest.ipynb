{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b5bd79b1ab577c3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix , classification_report"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T20:35:10.734433400Z",
     "start_time": "2024-03-04T20:35:10.713235900Z"
    }
   },
   "id": "cf4e5dc19998dc67",
   "execution_count": 179
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Dataset preparation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7a0b025d10136bc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "directory = 'animals'\n",
    "filepaths =[]\n",
    "labels=[]\n",
    "folders = os.listdir(directory)\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(directory, folder)\n",
    "    filenames= os.listdir(folder_path)\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        filepaths.append(filepath)\n",
    "        labels.append(folder)\n",
    "        \n",
    "file_series = pd.Series(filepaths , name='filepaths')\n",
    "label_series = pd.Series(labels , name='labels')\n",
    "element_series = pd.concat([file_series , label_series], axis=1)\n",
    "# Modify the labels for one-vs-rest classification\n",
    "element_series['labels_modified'] = np.where(element_series['labels'] == 'antelope', \"1\", \"0\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T20:35:10.790427600Z",
     "start_time": "2024-03-04T20:35:10.744999500Z"
    }
   },
   "id": "dfcdcce049dffcfa",
   "execution_count": 180
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the model using cnn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4647c4d356520144"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cnn = tf.keras.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64,64,3))) #1st convolutional layer\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size = 2, strides = 2))\n",
    "cnn.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dropout(0.5))  # Adding dropout for regularization\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dropout(0.5))  # Adding dropout for regularization\n",
    "\n",
    "# Output layer with softmax activation for multiclass classification\n",
    "cnn.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T20:35:10.989060200Z",
     "start_time": "2024-03-04T20:35:10.801642Z"
    }
   },
   "id": "125e3623856d0edd",
   "execution_count": 181
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_image(filepath, target_size=(224, 224)):\n",
    "    img = Image.open(filepath)\n",
    "    img = img.resize(target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T20:35:11.005449500Z",
     "start_time": "2024-03-04T20:35:10.991078700Z"
    }
   },
   "id": "869124e179fbf479",
   "execution_count": 182
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Applying 3 fold cross verification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5670569ce4b14578"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Training dataset size: 3600\n",
      "Validation dataset size: 1800\n",
      "Found 3600 validated image filenames belonging to 2 classes.\n",
      "Found 1800 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "113/113 [==============================] - 49s 431ms/step - loss: 0.0975 - accuracy: 0.9861 - val_loss: 0.0740 - val_accuracy: 0.9889\n",
      "Epoch 2/25\n",
      "113/113 [==============================] - 48s 424ms/step - loss: 0.0775 - accuracy: 0.9889 - val_loss: 0.0544 - val_accuracy: 0.9889\n",
      "Epoch 3/25\n",
      "113/113 [==============================] - 65s 576ms/step - loss: 0.0625 - accuracy: 0.9889 - val_loss: 0.0533 - val_accuracy: 0.9889\n",
      "Epoch 4/25\n",
      "113/113 [==============================] - 76s 675ms/step - loss: 0.1613 - accuracy: 0.9889 - val_loss: 0.0679 - val_accuracy: 0.9889\n",
      "Epoch 5/25\n",
      "113/113 [==============================] - 42s 372ms/step - loss: 0.0767 - accuracy: 0.9889 - val_loss: 0.0536 - val_accuracy: 0.9889\n",
      "Epoch 6/25\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.0661 - accuracy: 0.9889 - val_loss: 0.0766 - val_accuracy: 0.9889\n",
      "Epoch 7/25\n",
      "113/113 [==============================] - 46s 406ms/step - loss: 0.0921 - accuracy: 0.9889 - val_loss: 0.0683 - val_accuracy: 0.9889\n",
      "Epoch 8/25\n",
      "113/113 [==============================] - 44s 387ms/step - loss: 0.0656 - accuracy: 0.9889 - val_loss: 0.0617 - val_accuracy: 0.9889\n",
      "Epoch 9/25\n",
      "113/113 [==============================] - 80s 710ms/step - loss: 0.0629 - accuracy: 0.9889 - val_loss: 0.0563 - val_accuracy: 0.9889\n",
      "Epoch 10/25\n",
      "113/113 [==============================] - 57s 502ms/step - loss: 0.0640 - accuracy: 0.9889 - val_loss: 0.0555 - val_accuracy: 0.9889\n",
      "Epoch 11/25\n",
      "113/113 [==============================] - 42s 371ms/step - loss: 0.0673 - accuracy: 0.9889 - val_loss: 0.0588 - val_accuracy: 0.9889\n",
      "Epoch 12/25\n",
      "113/113 [==============================] - 42s 370ms/step - loss: 0.0735 - accuracy: 0.9889 - val_loss: 0.0784 - val_accuracy: 0.9889\n",
      "Epoch 13/25\n",
      "113/113 [==============================] - 39s 345ms/step - loss: 0.0600 - accuracy: 0.9889 - val_loss: 0.0574 - val_accuracy: 0.9889\n",
      "Epoch 14/25\n",
      "113/113 [==============================] - 39s 349ms/step - loss: 0.0671 - accuracy: 0.9889 - val_loss: 0.0580 - val_accuracy: 0.9889\n",
      "Epoch 15/25\n",
      "113/113 [==============================] - 39s 347ms/step - loss: 0.0638 - accuracy: 0.9889 - val_loss: 0.0547 - val_accuracy: 0.9889\n",
      "Epoch 16/25\n",
      "113/113 [==============================] - 40s 353ms/step - loss: 0.0883 - accuracy: 0.9889 - val_loss: 0.0599 - val_accuracy: 0.9889\n",
      "Epoch 17/25\n",
      "113/113 [==============================] - 39s 348ms/step - loss: 0.0690 - accuracy: 0.9889 - val_loss: 0.0630 - val_accuracy: 0.9889\n",
      "Epoch 18/25\n",
      "113/113 [==============================] - 39s 344ms/step - loss: 0.0657 - accuracy: 0.9889 - val_loss: 0.0623 - val_accuracy: 0.9889\n",
      "Epoch 19/25\n",
      "113/113 [==============================] - 39s 343ms/step - loss: 0.0645 - accuracy: 0.9889 - val_loss: 0.0616 - val_accuracy: 0.9889\n",
      "Epoch 20/25\n",
      "113/113 [==============================] - 39s 347ms/step - loss: 0.0661 - accuracy: 0.9889 - val_loss: 0.0636 - val_accuracy: 0.9889\n",
      "Epoch 21/25\n",
      "113/113 [==============================] - 45s 399ms/step - loss: 0.0649 - accuracy: 0.9889 - val_loss: 0.0609 - val_accuracy: 0.9889\n",
      "Epoch 22/25\n",
      "113/113 [==============================] - 46s 406ms/step - loss: 0.0642 - accuracy: 0.9889 - val_loss: 0.0579 - val_accuracy: 0.9889\n",
      "Epoch 23/25\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.0851 - accuracy: 0.9889 - val_loss: 0.0612 - val_accuracy: 0.9889\n",
      "Epoch 24/25\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.0651 - accuracy: 0.9889 - val_loss: 0.0590 - val_accuracy: 0.9889\n",
      "Epoch 25/25\n",
      "113/113 [==============================] - 53s 471ms/step - loss: 0.0625 - accuracy: 0.9889 - val_loss: 0.0615 - val_accuracy: 0.9889\n",
      "57/57 [==============================] - 14s 238ms/step - loss: 0.0615 - accuracy: 0.9889\n",
      "Validation Accuracy: 0.9888888597488403\n",
      "57/57 [==============================] - 13s 218ms/step\n",
      "Validation_set Accuracy: [[0.00664143]\n",
      " [0.00664143]\n",
      " [0.00279326]\n",
      " ...\n",
      " [0.00664143]\n",
      " [0.00664143]\n",
      " [0.00664143]]\n",
      "Confusion Matrix for Fold 1:\n",
      "[[1780    0]\n",
      " [  20    0]]\n",
      "\n",
      "Fold 2:\n",
      "Training dataset size: 3600\n",
      "Validation dataset size: 1800\n",
      "Found 3600 validated image filenames belonging to 2 classes.\n",
      "Found 1800 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "113/113 [==============================] - 48s 426ms/step - loss: 0.0711 - accuracy: 0.9889 - val_loss: 0.0594 - val_accuracy: 0.9889\n",
      "Epoch 2/25\n",
      "113/113 [==============================] - 45s 401ms/step - loss: 0.0633 - accuracy: 0.9889 - val_loss: 0.0582 - val_accuracy: 0.9889\n",
      "Epoch 3/25\n",
      "113/113 [==============================] - 46s 404ms/step - loss: 0.0625 - accuracy: 0.9889 - val_loss: 0.0566 - val_accuracy: 0.9889\n",
      "Epoch 4/25\n",
      "113/113 [==============================] - 44s 386ms/step - loss: 0.0656 - accuracy: 0.9889 - val_loss: 0.0589 - val_accuracy: 0.9889\n",
      "Epoch 5/25\n",
      "113/113 [==============================] - 47s 415ms/step - loss: 0.0651 - accuracy: 0.9889 - val_loss: 0.0584 - val_accuracy: 0.9889\n",
      "Epoch 6/25\n",
      "113/113 [==============================] - 45s 400ms/step - loss: 0.0629 - accuracy: 0.9889 - val_loss: 0.0551 - val_accuracy: 0.9889\n",
      "Epoch 7/25\n",
      "113/113 [==============================] - 49s 436ms/step - loss: 0.0635 - accuracy: 0.9889 - val_loss: 0.0611 - val_accuracy: 0.9889\n",
      "Epoch 8/25\n",
      "113/113 [==============================] - 48s 422ms/step - loss: 0.0609 - accuracy: 0.9889 - val_loss: 0.0570 - val_accuracy: 0.9889\n",
      "Epoch 9/25\n",
      "113/113 [==============================] - 54s 482ms/step - loss: 0.0627 - accuracy: 0.9889 - val_loss: 0.0599 - val_accuracy: 0.9889\n",
      "Epoch 10/25\n",
      "113/113 [==============================] - 49s 436ms/step - loss: 0.0616 - accuracy: 0.9889 - val_loss: 0.0568 - val_accuracy: 0.9889\n",
      "Epoch 11/25\n",
      "113/113 [==============================] - 45s 402ms/step - loss: 0.0643 - accuracy: 0.9889 - val_loss: 0.0589 - val_accuracy: 0.9889\n",
      "Epoch 12/25\n",
      "113/113 [==============================] - 40s 358ms/step - loss: 0.0901 - accuracy: 0.9889 - val_loss: 0.0603 - val_accuracy: 0.9889\n",
      "Epoch 13/25\n",
      "113/113 [==============================] - 42s 371ms/step - loss: 0.0631 - accuracy: 0.9889 - val_loss: 0.0571 - val_accuracy: 0.9889\n",
      "Epoch 14/25\n",
      "113/113 [==============================] - 48s 430ms/step - loss: 0.0689 - accuracy: 0.9889 - val_loss: 0.0599 - val_accuracy: 0.9889\n",
      "Epoch 15/25\n",
      "113/113 [==============================] - 48s 427ms/step - loss: 0.0636 - accuracy: 0.9889 - val_loss: 0.0584 - val_accuracy: 0.9889\n",
      "Epoch 16/25\n",
      "113/113 [==============================] - 42s 374ms/step - loss: 0.0609 - accuracy: 0.9889 - val_loss: 0.0573 - val_accuracy: 0.9889\n",
      "Epoch 17/25\n",
      "113/113 [==============================] - 60s 532ms/step - loss: 0.0634 - accuracy: 0.9889 - val_loss: 0.0589 - val_accuracy: 0.9889\n",
      "Epoch 18/25\n",
      "113/113 [==============================] - 55s 491ms/step - loss: 0.0647 - accuracy: 0.9889 - val_loss: 0.0625 - val_accuracy: 0.9889\n",
      "Epoch 19/25\n",
      "113/113 [==============================] - 58s 513ms/step - loss: 0.0611 - accuracy: 0.9889 - val_loss: 0.0593 - val_accuracy: 0.9889\n",
      "Epoch 20/25\n",
      "113/113 [==============================] - 65s 578ms/step - loss: 0.0599 - accuracy: 0.9889 - val_loss: 0.0588 - val_accuracy: 0.9889\n",
      "Epoch 21/25\n",
      "113/113 [==============================] - 68s 597ms/step - loss: 0.0641 - accuracy: 0.9889 - val_loss: 0.0590 - val_accuracy: 0.9889\n",
      "Epoch 22/25\n",
      "113/113 [==============================] - 49s 430ms/step - loss: 0.0623 - accuracy: 0.9889 - val_loss: 0.0584 - val_accuracy: 0.9889\n",
      "Epoch 23/25\n",
      "113/113 [==============================] - 54s 474ms/step - loss: 0.0658 - accuracy: 0.9889 - val_loss: 0.0587 - val_accuracy: 0.9889\n",
      "Epoch 24/25\n",
      "113/113 [==============================] - 54s 482ms/step - loss: 0.0592 - accuracy: 0.9889 - val_loss: 0.0577 - val_accuracy: 0.9889\n",
      "Epoch 25/25\n",
      "113/113 [==============================] - 45s 398ms/step - loss: 0.0604 - accuracy: 0.9889 - val_loss: 0.0554 - val_accuracy: 0.9889\n",
      "57/57 [==============================] - 13s 221ms/step - loss: 0.0554 - accuracy: 0.9889\n",
      "Validation Accuracy: 0.9888888597488403\n",
      "57/57 [==============================] - 14s 240ms/step\n",
      "Validation_set Accuracy: [[0.00091193]\n",
      " [0.01562356]\n",
      " [0.00065467]\n",
      " ...\n",
      " [0.00230942]\n",
      " [0.02271188]\n",
      " [0.01516301]]\n",
      "Confusion Matrix for Fold 2:\n",
      "[[1780    0]\n",
      " [  20    0]]\n",
      "\n",
      "Fold 3:\n",
      "Training dataset size: 3600\n",
      "Validation dataset size: 1800\n",
      "Found 3600 validated image filenames belonging to 2 classes.\n",
      "Found 1800 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "113/113 [==============================] - 61s 541ms/step - loss: 0.0641 - accuracy: 0.9889 - val_loss: 0.0582 - val_accuracy: 0.9889\n",
      "Epoch 2/25\n",
      " 32/113 [=======>......................] - ETA: 31s - loss: 0.0789 - accuracy: 0.9834"
     ]
    }
   ],
   "source": [
    "n_splits =3\n",
    "skf = StratifiedKFold(n_splits=n_splits , random_state= 42, shuffle=True)\n",
    "fold_indices = list(skf.split(element_series['filepaths'], element_series['labels']))\n",
    "for fold , (train_index, val_index)in enumerate(fold_indices):\n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "    train_fold_df = element_series.iloc[train_index]\n",
    "    val_fold_df = element_series.iloc[val_index]\n",
    "    print(\"Training dataset size:\", len(train_fold_df))\n",
    "    print(\"Validation dataset size:\", len(val_fold_df))\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Validation data should only be rescaled\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Create data generators for train and validation sets\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_fold_df,\n",
    "        x_col='filepaths',\n",
    "        y_col='labels_modified',\n",
    "        target_size=(64, 64),  # Adjust according to your model's input shape\n",
    "        batch_size=32,\n",
    "        class_mode='binary'  # or 'categorical' for multiclass classification\n",
    "    )\n",
    "    \n",
    "    validation_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=val_fold_df,\n",
    "        x_col='filepaths',\n",
    "        y_col='labels_modified',\n",
    "        target_size=(64, 64),  # Adjust according to your model's input shape\n",
    "        batch_size=32,\n",
    "        class_mode='binary'  # or 'categorical' for multiclass classification\n",
    "    )\n",
    "    \n",
    "    # Train your model using the data generators\n",
    "    history = cnn.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator)\n",
    "    )\n",
    "    # Evaluate accuracy\n",
    "    val_loss, val_acc = cnn.evaluate(validation_generator)\n",
    "    print(f\"Validation Accuracy: {val_acc}\")\n",
    "\n",
    "    val_labels = val_fold_df['labels_modified']\n",
    "\n",
    "\n",
    "\n",
    "    # Predict labels for validation data\n",
    "    val_pred = cnn.predict(validation_generator)\n",
    "    print(f\"Validation_set Accuracy: {val_pred}\")\n",
    "    val_pred = np.argmax(val_pred, axis=1)\n",
    "    # Convert predicted labels to strings\n",
    "    val_pred_str = val_pred.astype(str)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(val_labels, val_pred_str)\n",
    "    print(f\"Confusion Matrix for Fold {fold + 1}:\\n{cm}\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-04T20:35:11.006497900Z"
    }
   },
   "id": "ac489c30cd7ad6",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
