{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b5bd79b1ab577c3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix , classification_report"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T16:28:30.263122300Z",
     "start_time": "2024-03-04T16:28:30.247995700Z"
    }
   },
   "id": "cf4e5dc19998dc67",
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Dataset preparation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7a0b025d10136bc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "directory = 'animals'\n",
    "filepaths =[]\n",
    "labels=[]\n",
    "folders = os.listdir(directory)\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(directory, folder)\n",
    "    filenames= os.listdir(folder_path)\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        filepaths.append(filepath)\n",
    "        labels.append(folder)\n",
    "        \n",
    "file_series = pd.Series(filepaths , name='filepaths')\n",
    "label_series = pd.Series(labels , name='labels')\n",
    "element_series = pd.concat([file_series , label_series], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T16:28:30.310412700Z",
     "start_time": "2024-03-04T16:28:30.272065500Z"
    }
   },
   "id": "dfcdcce049dffcfa",
   "execution_count": 115
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the model using cnn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4647c4d356520144"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cnn = tf.keras.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64,64,3))) #1st convolutional layer\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size = 2, strides = 2))\n",
    "cnn.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dropout(0.5))  # Adding dropout for regularization\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dropout(0.5))  # Adding dropout for regularization\n",
    "\n",
    "# Output layer with softmax activation for multiclass classification\n",
    "cnn.add(tf.keras.layers.Dense(90, activation='softmax'))\n",
    "\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T16:28:30.505838100Z",
     "start_time": "2024-03-04T16:28:30.320436900Z"
    }
   },
   "id": "125e3623856d0edd",
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_image(filepath, target_size=(224, 224)):\n",
    "    img = Image.open(filepath)\n",
    "    img = img.resize(target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T16:28:30.521836400Z",
     "start_time": "2024-03-04T16:28:30.509838Z"
    }
   },
   "id": "869124e179fbf479",
   "execution_count": 117
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Applying 3 fold cross verification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5670569ce4b14578"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Training dataset size: 3600\n",
      "Validation dataset size: 1800\n",
      "Found 3600 validated image filenames belonging to 90 classes.\n",
      "Found 1800 validated image filenames belonging to 90 classes.\n",
      "Epoch 1/20\n",
      "113/113 [==============================] - 63s 541ms/step - loss: 0.1080 - accuracy: 0.0106 - val_loss: 0.0621 - val_accuracy: 0.0111\n",
      "Epoch 2/20\n",
      "113/113 [==============================] - 65s 568ms/step - loss: 0.0681 - accuracy: 0.0161 - val_loss: 0.0612 - val_accuracy: 0.0156\n",
      "Epoch 3/20\n",
      "113/113 [==============================] - 41s 365ms/step - loss: 0.0664 - accuracy: 0.0108 - val_loss: 0.0615 - val_accuracy: 0.0189\n",
      "Epoch 4/20\n",
      "113/113 [==============================] - 42s 370ms/step - loss: 0.0651 - accuracy: 0.0156 - val_loss: 0.0613 - val_accuracy: 0.0206\n",
      "Epoch 5/20\n",
      "113/113 [==============================] - 42s 374ms/step - loss: 0.0642 - accuracy: 0.0156 - val_loss: 0.0610 - val_accuracy: 0.0194\n",
      "Epoch 6/20\n",
      "113/113 [==============================] - 42s 369ms/step - loss: 0.0635 - accuracy: 0.0200 - val_loss: 0.0599 - val_accuracy: 0.0244\n",
      "Epoch 7/20\n",
      "113/113 [==============================] - 41s 368ms/step - loss: 0.0630 - accuracy: 0.0206 - val_loss: 0.0600 - val_accuracy: 0.0228\n",
      "Epoch 8/20\n",
      "113/113 [==============================] - 41s 365ms/step - loss: 0.0625 - accuracy: 0.0172 - val_loss: 0.0597 - val_accuracy: 0.0244\n",
      "Epoch 9/20\n",
      "113/113 [==============================] - 41s 366ms/step - loss: 0.0625 - accuracy: 0.0192 - val_loss: 0.0597 - val_accuracy: 0.0250\n",
      "Epoch 10/20\n",
      "113/113 [==============================] - 42s 370ms/step - loss: 0.0620 - accuracy: 0.0222 - val_loss: 0.0596 - val_accuracy: 0.0289\n",
      "Epoch 11/20\n",
      "113/113 [==============================] - 43s 382ms/step - loss: 0.0620 - accuracy: 0.0231 - val_loss: 0.0593 - val_accuracy: 0.0356\n",
      "Epoch 12/20\n",
      "113/113 [==============================] - 42s 375ms/step - loss: 0.0612 - accuracy: 0.0272 - val_loss: 0.0587 - val_accuracy: 0.0350\n",
      "Epoch 13/20\n",
      "113/113 [==============================] - 42s 369ms/step - loss: 0.0609 - accuracy: 0.0328 - val_loss: 0.0580 - val_accuracy: 0.0500\n",
      "Epoch 14/20\n",
      "113/113 [==============================] - 44s 390ms/step - loss: 0.0601 - accuracy: 0.0364 - val_loss: 0.0578 - val_accuracy: 0.0406\n",
      "Epoch 15/20\n",
      "113/113 [==============================] - 42s 375ms/step - loss: 0.0595 - accuracy: 0.0417 - val_loss: 0.0574 - val_accuracy: 0.0528\n",
      "Epoch 16/20\n",
      "113/113 [==============================] - 43s 380ms/step - loss: 0.0590 - accuracy: 0.0408 - val_loss: 0.0563 - val_accuracy: 0.0594\n",
      "Epoch 17/20\n",
      "113/113 [==============================] - 42s 373ms/step - loss: 0.0584 - accuracy: 0.0458 - val_loss: 0.0573 - val_accuracy: 0.0544\n",
      "Epoch 18/20\n",
      "113/113 [==============================] - 42s 369ms/step - loss: 0.0578 - accuracy: 0.0544 - val_loss: 0.0571 - val_accuracy: 0.0611\n",
      "Epoch 19/20\n",
      "113/113 [==============================] - 43s 378ms/step - loss: 0.0573 - accuracy: 0.0569 - val_loss: 0.0556 - val_accuracy: 0.0611\n",
      "Epoch 20/20\n",
      "113/113 [==============================] - 42s 371ms/step - loss: 0.0570 - accuracy: 0.0617 - val_loss: 0.0548 - val_accuracy: 0.0733\n",
      "57/57 [==============================] - 12s 211ms/step - loss: 0.0548 - accuracy: 0.0733\n",
      "Validation Accuracy: 0.07333333045244217\n",
      "57/57 [==============================] - 12s 210ms/step\n",
      "Confusion Matrix for Fold 1:\n",
      "[[0 2 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Fold 2:\n",
      "Training dataset size: 3600\n",
      "Validation dataset size: 1800\n",
      "Found 3600 validated image filenames belonging to 90 classes.\n",
      "Found 1800 validated image filenames belonging to 90 classes.\n",
      "Epoch 1/20\n",
      "113/113 [==============================] - 43s 382ms/step - loss: 0.0569 - accuracy: 0.0561 - val_loss: 0.0529 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "113/113 [==============================] - 43s 379ms/step - loss: 0.0564 - accuracy: 0.0700 - val_loss: 0.0525 - val_accuracy: 0.1194\n",
      "Epoch 3/20\n",
      "113/113 [==============================] - 43s 384ms/step - loss: 0.0557 - accuracy: 0.0672 - val_loss: 0.0527 - val_accuracy: 0.1056\n",
      "Epoch 4/20\n",
      "113/113 [==============================] - 43s 381ms/step - loss: 0.0555 - accuracy: 0.0753 - val_loss: 0.0529 - val_accuracy: 0.1033\n",
      "Epoch 5/20\n",
      "113/113 [==============================] - 42s 372ms/step - loss: 0.0550 - accuracy: 0.0803 - val_loss: 0.0514 - val_accuracy: 0.1222\n",
      "Epoch 6/20\n",
      "113/113 [==============================] - 43s 382ms/step - loss: 0.0546 - accuracy: 0.0911 - val_loss: 0.0517 - val_accuracy: 0.1300\n",
      "Epoch 7/20\n",
      "113/113 [==============================] - 43s 381ms/step - loss: 0.0543 - accuracy: 0.0933 - val_loss: 0.0511 - val_accuracy: 0.1250\n",
      "Epoch 8/20\n",
      "113/113 [==============================] - 44s 394ms/step - loss: 0.0543 - accuracy: 0.0903 - val_loss: 0.0513 - val_accuracy: 0.1278\n",
      "Epoch 9/20\n",
      "113/113 [==============================] - 44s 389ms/step - loss: 0.0538 - accuracy: 0.0961 - val_loss: 0.0503 - val_accuracy: 0.1456\n",
      "Epoch 10/20\n",
      "113/113 [==============================] - 44s 385ms/step - loss: 0.0533 - accuracy: 0.1119 - val_loss: 0.0508 - val_accuracy: 0.1294\n",
      "Epoch 11/20\n",
      "113/113 [==============================] - 42s 375ms/step - loss: 0.0531 - accuracy: 0.1047 - val_loss: 0.0499 - val_accuracy: 0.1561\n",
      "Epoch 12/20\n",
      "113/113 [==============================] - 43s 377ms/step - loss: 0.0529 - accuracy: 0.1081 - val_loss: 0.0509 - val_accuracy: 0.1478\n",
      "Epoch 13/20\n",
      "113/113 [==============================] - 43s 379ms/step - loss: 0.0527 - accuracy: 0.1142 - val_loss: 0.0501 - val_accuracy: 0.1572\n",
      "Epoch 14/20\n",
      "113/113 [==============================] - 43s 382ms/step - loss: 0.0522 - accuracy: 0.1217 - val_loss: 0.0492 - val_accuracy: 0.1628\n",
      "Epoch 15/20\n",
      "113/113 [==============================] - 44s 387ms/step - loss: 0.0518 - accuracy: 0.1258 - val_loss: 0.0486 - val_accuracy: 0.1817\n",
      "Epoch 16/20\n",
      "113/113 [==============================] - 43s 381ms/step - loss: 0.0513 - accuracy: 0.1361 - val_loss: 0.0512 - val_accuracy: 0.1428\n",
      "Epoch 17/20\n",
      "113/113 [==============================] - 42s 374ms/step - loss: 0.0513 - accuracy: 0.1397 - val_loss: 0.0481 - val_accuracy: 0.1822\n",
      "Epoch 18/20\n",
      "113/113 [==============================] - 43s 377ms/step - loss: 0.0506 - accuracy: 0.1556 - val_loss: 0.0491 - val_accuracy: 0.1733\n",
      "Epoch 19/20\n",
      "113/113 [==============================] - 43s 377ms/step - loss: 0.0505 - accuracy: 0.1483 - val_loss: 0.0473 - val_accuracy: 0.2183\n",
      "Epoch 20/20\n",
      "113/113 [==============================] - 43s 381ms/step - loss: 0.0502 - accuracy: 0.1567 - val_loss: 0.0475 - val_accuracy: 0.1917\n",
      "57/57 [==============================] - 13s 226ms/step - loss: 0.0475 - accuracy: 0.1917\n",
      "Validation Accuracy: 0.19166666269302368\n",
      "57/57 [==============================] - 13s 229ms/step\n",
      "Confusion Matrix for Fold 2:\n",
      "[[0 0 0 ... 0 2 1]\n",
      " [0 0 0 ... 2 1 3]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 1 ... 0 0 3]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "\n",
      "Fold 3:\n",
      "Training dataset size: 3600\n",
      "Validation dataset size: 1800\n",
      "Found 3600 validated image filenames belonging to 90 classes.\n",
      "Found 1800 validated image filenames belonging to 90 classes.\n",
      "Epoch 1/20\n",
      "113/113 [==============================] - 44s 387ms/step - loss: 0.0510 - accuracy: 0.1464 - val_loss: 0.0452 - val_accuracy: 0.2344\n",
      "Epoch 2/20\n",
      "113/113 [==============================] - 42s 377ms/step - loss: 0.0504 - accuracy: 0.1536 - val_loss: 0.0484 - val_accuracy: 0.1861\n",
      "Epoch 3/20\n",
      "113/113 [==============================] - 43s 379ms/step - loss: 0.0501 - accuracy: 0.1594 - val_loss: 0.0457 - val_accuracy: 0.2311\n",
      "Epoch 4/20\n",
      "113/113 [==============================] - 43s 378ms/step - loss: 0.0493 - accuracy: 0.1761 - val_loss: 0.0444 - val_accuracy: 0.2439\n",
      "Epoch 5/20\n",
      "113/113 [==============================] - 43s 378ms/step - loss: 0.0492 - accuracy: 0.1861 - val_loss: 0.0449 - val_accuracy: 0.2411\n",
      "Epoch 6/20\n",
      "113/113 [==============================] - 43s 382ms/step - loss: 0.0486 - accuracy: 0.1817 - val_loss: 0.0444 - val_accuracy: 0.2556\n",
      "Epoch 7/20\n",
      "113/113 [==============================] - 43s 386ms/step - loss: 0.0486 - accuracy: 0.1789 - val_loss: 0.0442 - val_accuracy: 0.2572\n",
      "Epoch 8/20\n",
      "113/113 [==============================] - 43s 380ms/step - loss: 0.0482 - accuracy: 0.1867 - val_loss: 0.0458 - val_accuracy: 0.2256\n",
      "Epoch 9/20\n",
      "113/113 [==============================] - 42s 373ms/step - loss: 0.0479 - accuracy: 0.1947 - val_loss: 0.0473 - val_accuracy: 0.2089\n",
      "Epoch 10/20\n",
      "113/113 [==============================] - 42s 373ms/step - loss: 0.0473 - accuracy: 0.2081 - val_loss: 0.0457 - val_accuracy: 0.2378\n",
      "Epoch 11/20\n",
      "113/113 [==============================] - 43s 380ms/step - loss: 0.0469 - accuracy: 0.2167 - val_loss: 0.0472 - val_accuracy: 0.2078\n",
      "Epoch 12/20\n",
      "113/113 [==============================] - 43s 379ms/step - loss: 0.0473 - accuracy: 0.2003 - val_loss: 0.0449 - val_accuracy: 0.2361\n",
      "Epoch 13/20\n",
      "113/113 [==============================] - 44s 386ms/step - loss: 0.0468 - accuracy: 0.2089 - val_loss: 0.0453 - val_accuracy: 0.2444\n",
      "Epoch 14/20\n",
      "113/113 [==============================] - 44s 387ms/step - loss: 0.0464 - accuracy: 0.2117 - val_loss: 0.0471 - val_accuracy: 0.2250\n",
      "Epoch 15/20\n",
      "113/113 [==============================] - 43s 382ms/step - loss: 0.0458 - accuracy: 0.2222 - val_loss: 0.0474 - val_accuracy: 0.2256\n",
      "Epoch 16/20\n",
      "113/113 [==============================] - 42s 375ms/step - loss: 0.0459 - accuracy: 0.2350 - val_loss: 0.0528 - val_accuracy: 0.1789\n",
      "Epoch 17/20\n",
      "113/113 [==============================] - 43s 378ms/step - loss: 0.0457 - accuracy: 0.2275 - val_loss: 0.0472 - val_accuracy: 0.2250\n",
      "Epoch 18/20\n",
      "113/113 [==============================] - 43s 378ms/step - loss: 0.0454 - accuracy: 0.2306 - val_loss: 0.0490 - val_accuracy: 0.2272\n",
      "Epoch 19/20\n",
      "113/113 [==============================] - 44s 386ms/step - loss: 0.0451 - accuracy: 0.2436 - val_loss: 0.0510 - val_accuracy: 0.2117\n",
      "Epoch 20/20\n",
      "113/113 [==============================] - 46s 405ms/step - loss: 0.0446 - accuracy: 0.2406 - val_loss: 0.0461 - val_accuracy: 0.2344\n",
      "57/57 [==============================] - 11s 190ms/step - loss: 0.0461 - accuracy: 0.2344\n",
      "Validation Accuracy: 0.23444443941116333\n",
      "57/57 [==============================] - 11s 191ms/step\n",
      "Confusion Matrix for Fold 3:\n",
      "[[1 0 0 ... 0 0 3]\n",
      " [0 0 1 ... 0 2 0]\n",
      " [0 0 1 ... 0 1 2]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 0 0 ... 0 0 3]\n",
      " [0 0 1 ... 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "n_splits =3\n",
    "skf = StratifiedKFold(n_splits=n_splits , random_state= 42, shuffle=True)\n",
    "fold_indices = list(skf.split(element_series['filepaths'], element_series['labels']))\n",
    "for fold , (train_index, val_index)in enumerate(fold_indices):\n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "    train_fold_df = element_series.iloc[train_index]\n",
    "    val_fold_df = element_series.iloc[val_index]\n",
    "    print(\"Training dataset size:\", len(train_fold_df))\n",
    "    print(\"Validation dataset size:\", len(val_fold_df))\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Validation data should only be rescaled\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Create data generators for train and validation sets\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_fold_df,\n",
    "        x_col='filepaths',\n",
    "        y_col='labels',\n",
    "        target_size=(64, 64),  # Adjust according to your model's input shape\n",
    "        batch_size=32,\n",
    "        class_mode='categorical'  # or 'categorical' for multiclass classification\n",
    "    )\n",
    "    \n",
    "    validation_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=val_fold_df,\n",
    "        x_col='filepaths',\n",
    "        y_col='labels',\n",
    "        target_size=(64, 64),  # Adjust according to your model's input shape\n",
    "        batch_size=32,\n",
    "        class_mode='categorical'  # or 'categorical' for multiclass classification\n",
    "    )\n",
    "    \n",
    "    # Train your model using the data generators\n",
    "    history = cnn.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=20,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator)\n",
    "    )\n",
    "    # Evaluate accuracy\n",
    "    val_loss, val_acc = cnn.evaluate(validation_generator)\n",
    "    print(f\"Validation Accuracy: {val_acc}\")\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    val_labels = np.array([validation_generator.class_indices[label] for label in val_fold_df['labels']])\n",
    "\n",
    "    # Predict labels for validation data\n",
    "    val_pred = cnn.predict(validation_generator)\n",
    "    val_pred = np.argmax(val_pred, axis=1)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(val_labels, val_pred)\n",
    "    print(f\"Confusion Matrix for Fold {fold + 1}:\\n{cm}\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T17:13:10.412714100Z",
     "start_time": "2024-03-04T16:28:30.535203900Z"
    }
   },
   "id": "ac489c30cd7ad6",
   "execution_count": 118
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
